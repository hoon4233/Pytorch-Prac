{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                   1.8.0\r\n",
      "torchsummary            1.5.1\r\n",
      "torchtext               0.3.1\r\n",
      "torchvision             0.9.1\r\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep \"torch\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Field'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-97-23389131178d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtorchtext\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m TEXT = data.Field(batch_first=True,\n\u001B[0m\u001B[1;32m      8\u001B[0m                   \u001B[0mfix_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m500\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m                   \u001B[0mtokenize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torchtext.data' has no attribute 'Field'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from torchtext.legacy import data\n",
    "# from torchtext.legacy import datasets\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  fix_length=500,\n",
    "                  tokenize=str.split,\n",
    "                  pad_first=True,\n",
    "                  pad_token='[PAD]',\n",
    "                  unk_token='[UNK]')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(text_field=TEXT, label_field=LABEL)\n",
    "\n",
    "# batch_first == batch size를 data shape axis의 가장 앞으로 설정\n",
    "# fix_length == sentence의 길이를 미리 제한하는 옵션\n",
    "# tokenize == tokenize를 설정하는 옵션, 여기서는 파이썬의 string.split (그냥 띄어쓰기로 짜름)\n",
    "# pad_first == fix_lenght 대비 짧은 문장의 경우 padding을 앞에서 줄 것인지에 대한 옵션\n",
    "# pad_token == padding을 할 때 사용할 token\n",
    "# unk_token == token dict에 없는 token이 나올 경우 token을 표현하는 특수 token\n",
    "# dtype == 가져올 데이터에 대한 type 설정 옵션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Train Data Length : {len(train_data.examples)}\")\n",
    "print(f\"Test Data Length : {len(test_data.examples)}\")\n",
    "\n",
    "print(train_data.fields)\n",
    "\n",
    "print('--- Data Sample ---')\n",
    "print('Input : ')\n",
    "print(' '.join(vars(train_data.examples[1])['text']),'\\n')\n",
    "print(f'Label : {vars(train_data.examples[1])[\"label\"]}')\n",
    "print(vars(train_data.examples[1])['text'],'\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def PreProcessingText(input_sentence) :\n",
    "    input_sentence = input_sentence.lower()\n",
    "    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) # \"<br />\" 처리\n",
    "    input_sentence = re.sub('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]', repl= ' ', string = input_sentence) # 특수문자 처리 (\"'\" 제외)\n",
    "    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리\n",
    "    if input_sentence:\n",
    "        return input_sentence\n",
    "\n",
    "for example in train_data.examples :\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n",
    "\n",
    "for example in test_data.examples :\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_config = {'emb_type' : 'glove', 'emb_dim' : 300}\n",
    "\n",
    "# pre_trained\n",
    "TEXT.build_vocab(train_data, min_freq=2, max_size=None, vectors = 'glove.6B.300d')\n",
    "# min_freq == vocab에 해당하는 token에 최소한으로 등자하는 횟수에 제한을 둔다.\n",
    "# max_size == vocab size 에 제한을 둔다\n",
    "# vecotrs == pre-trained vector를 가져와 vocab을 세팅한다.\n",
    "LABEL.build_vocab(train_data)\n",
    "model_config['vocab_size'] = len(TEXT.vocab)\n",
    "print(TEXT.vocab.stoi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Vocab size : {len(TEXT.vocab)}\")\n",
    "print('Vocab Examples :')\n",
    "for idx, (k, v) in enumerate(TEXT.vocab.stoi.items()):\n",
    "    if idx >= 10 :\n",
    "        break\n",
    "    print('\\t', k, v)\n",
    "print('---------------------')\n",
    "\n",
    "print(f'Label Size : {len(LABEL.vocab)}')\n",
    "print('Label Examples : ')\n",
    "for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n",
    "    print('\\t', k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check embedding vectors\n",
    "print(TEXT.vocab.vectors.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(0),\n",
    "                                          split_ratio=0.8)\n",
    "model_config['batch_size'] = 30\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    datasets=(train_data, valid_data, test_data), batch_size=model_config['batch_size'], device=device)\n",
    "# Bucket iterator로 손쉽게 Batch를 만들 수 있다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "sample_for_check = next(iter(train_iterator))\n",
    "print(sample_for_check)\n",
    "print(sample_for_check.text)\n",
    "print(sample_for_check.label)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentenceClassification(nn.Module) :\n",
    "    def __init__(self, **model_config):\n",
    "        super(SentenceClassification, self).__init__()\n",
    "\n",
    "        if model_config['emb_type'] == 'glove' or 'fasttext' :\n",
    "            self.emb = nn.Embedding(model_config['vocab_size'],\n",
    "                                    model_config['emb_dim'],\n",
    "                                    _weight=TEXT.vocab.vectors)\n",
    "            # 순서대로 num_embeddings, embedding_dim, _wieght\n",
    "            # num_embeddings == vocab_size\n",
    "            # embedding_dim == 원하는 embedding dimension (pre-trained vector 사용시 차원 일치시켜야  한다.)\n",
    "            # _weight == pre-trained 된 vector를 initial value로 설정\n",
    "                # 이렇게 하면 해당 임베딩 벡터도 같이 학습된다. freeze 시키고 싶으면 nn.Embedding.from_pretrained(TEXT.vocab.vectors) 사용\n",
    "        else :\n",
    "            self.emb = nn.Embedding(model_config['vocab_size'],\n",
    "                                    model_config['emb_dim'])\n",
    "\n",
    "        self.bidirectional = model_config['bidirectional']\n",
    "        self.num_direction = 2 if model_config['bidirectional'] else 1\n",
    "        self.model_type = model_config['model_type']\n",
    "\n",
    "        self.RNN = nn.RNN(input_size=model_config['emb_dim'],\n",
    "                          hidden_size=model_config['hidden_dim'],\n",
    "                          dropout=model_config['dropout'],\n",
    "                          bidirectional=model_config['bidirectional'],\n",
    "                          batch_first=model_config['batch_first'])\n",
    "        self.LSTM = nn.LSTM(input_size=model_config['emb_dim'],\n",
    "                          hidden_size=model_config['hidden_dim'],\n",
    "                          dropout=model_config['dropout'],\n",
    "                          bidirectional=model_config['bidirectional'],\n",
    "                          batch_first=model_config['batch_first'])\n",
    "        self.GRU = nn.GRU(input_size=model_config['emb_dim'],\n",
    "                          hidden_size=model_config['hidden_dim'],\n",
    "                          dropout=model_config['dropout'],\n",
    "                          bidirectional=model_config['bidirectional'],\n",
    "                          batch_first=model_config['batch_first'])\n",
    "        self.fc = nn.Linear(model_config['hidden_dim']*self.num_direction,\n",
    "                            model_config['output_dim'])\n",
    "        self.drop = nn.Dropout(model_config['dropout'])\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x : (Batch_size, max_seq_length)\n",
    "        # emb : (Batch_size, max_seq_length, emb_dim)\n",
    "        # output : (Batch_size, max_seq_length, hidden_dim * num_direction)\n",
    "        # hidden : (num_direction, batch_size, hidden_dim)\n",
    "        # last_output : (batch_size, hidden_dim * num_direction)\n",
    "\n",
    "        emb = self.emb(x)\n",
    "        if self.model_type == 'RNN':\n",
    "            output, hidden = self.RNN(emb)\n",
    "        elif self.model_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.LSTM(emb)\n",
    "        elif self.model_type == 'GRU':\n",
    "            output, hidden = self.GRU(emb)\n",
    "        else :\n",
    "            raise NameError('Select model_type in [RNN, LSTM, GRU]')\n",
    "        last_output = output[:,-1,:] # token의 위치를 설명하는 두 번째 차원에서 마지막 값을 가져와 사용\n",
    "\n",
    "        return self.fc(self.drop(last_output))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_config.update(dict(batch_first = True,\n",
    "                         model_type = 'RNN',\n",
    "                         bidirectional = True,\n",
    "                         hidden_dim = 128,\n",
    "                         output_dim = 1,\n",
    "                         dropout = 0))\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "# 내부에 sigmoid layer를 통과시키기 때문에 model 구성시 sigmoid를 따로 사용하지 않는다.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc =correct.sum()/len(correct)\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = model.forward(sample_for_check.text).squeeze()\n",
    "loss = loss_fn(predictions, sample_for_check.label)\n",
    "acc = binary_accuracy(predictions, sample_for_check.label)\n",
    "\n",
    "print(predictions)\n",
    "print(loss, acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train(model, iterator, optimizer, loss_fn, idx_epoch, **model_params):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    batch_size = model_params['batch_size']\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "\n",
    "        # Initializing\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        predictions = model(batch.text).squeeze()\n",
    "        loss = loss_fn(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        sys.stdout.write(\n",
    "                    \"\\r\" + f\"[Train] Epoch : {idx_epoch:^3}\"\\\n",
    "                    f\"[{(idx + 1) * batch_size} / {len(iterator) * batch_size} ({100. * (idx + 1) / len(iterator) :.4}%)]\"\\\n",
    "                    f\"  Loss: {loss.item():.4}\"\\\n",
    "                    f\"  Acc : {acc.item():.4}\"\\\n",
    "                    )\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update Epoch Performance\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss/len(iterator) , epoch_acc/len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, iterator, loss_fn):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = loss_fn(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'RNN'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "N_EPOCH = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss: # early stopping 기능임\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}